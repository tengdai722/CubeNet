{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npath = 'OpenIE/Stanford-OpenIE/nyt'\\nf = 'segmentation'\\n\\nsegOutput = open(path +'/train.seg.txt', 'w')\\n\\nwith open(path +'/'+  f + '.txt','r') as thefile:\\n    for line in thefile:\\n        pointer=1\\n        in_phrase = False\\n        st=[]\\n        arr = line.rstrip('\\n').split(' ')\\n        for word in arr:\\n            if '<phrase>' in word:\\n                in_phrase = True\\n            if in_phrase :    \\n                st.append(str(pointer))\\n            else:\\n                st.append('O')\\n            if '</phrase>' in word:\\n                in_phrase = False\\n                pointer+=1\\n        sentence=' '.join(st)\\n        segOutput.write(sentence + '\\n')\\nsegOutput.close()       \\n\\n#%%\\n\\ndata_all=[]\\nread_file =['tokens.', 'pos.', 'ner.' , 'dep.', 'seg.']\\nfor f in read_file:\\n    thefile = open(path + '/train.' + f + 'txt','r')\\n    i=0\\n    for line in thefile:\\n        arr = line.rstrip('\\n').split(' ')\\n        if not data_all:\\n            data_all= [[] for t in range(235972)]\\n        if not data_all[i]:\\n            data_all[i] = [[] for t in range(len(arr))]\\n        for j,ele in enumerate(arr):\\n            data_all[i][j].append(ele)\\n        i=i+1\\n# merge phrases <Hu Jintao>-><Hu_Jintao>, not PERSON PERSON, Hu PERSON, PERSON Jintao\\nf='phrase.'\\nthefile = open(path + '/train.' + f + 'txt','w')\\nmerged_data=[[] for i in range(len(data_all))]\\nfor j, sentence in enumerate(data_all):\\n    for i, word in enumerate(sentence):\\n        newWord=word[0]\\n        if (word[2] == 'O' or word[2] == 'MISC') and word[4] =='O' or i==0:\\n            merged_data[j].append(newWord)\\n            continue\\n        else:\\n            if word[4] !='O' and word[4] !=sentence[i-1][4] and (word[2] == 'O' or word[2] == 'MISC'):\\n                merged_data[j].append(newWord)\\n            elif  word[4] !='O' and word[4] ==sentence[i-1][4] or (word[2] != 'O' and word[2] != 'MISC' and word[2] ==sentence[i-1][2]):\\n                merged_data[j][-1]='_'.join((merged_data[j][-1],newWord))\\n            elif word[2] != 'O' and word[2] != 'MISC' and word[2] !=sentence[i-1][2] :\\n                newWord='_'.join((word[2],newWord))\\n                merged_data[j].append(newWord)\\n    merged_data[j]=' '.join(merged_data[j])\\n    thefile.write(merged_data[j]+'\\n')\\n\""
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "path = 'OpenIE/Stanford-OpenIE/nyt'\n",
    "f = 'segmentation'\n",
    "\n",
    "segOutput = open(path +'/train.seg.txt', 'w')\n",
    "\n",
    "with open(path +'/'+  f + '.txt','r') as thefile:\n",
    "    for line in thefile:\n",
    "        pointer=1\n",
    "        in_phrase = False\n",
    "        st=[]\n",
    "        arr = line.rstrip('\\n').split(' ')\n",
    "        for word in arr:\n",
    "            if '<phrase>' in word:\n",
    "                in_phrase = True\n",
    "            if in_phrase :    \n",
    "                st.append(str(pointer))\n",
    "            else:\n",
    "                st.append('O')\n",
    "            if '</phrase>' in word:\n",
    "                in_phrase = False\n",
    "                pointer+=1\n",
    "        sentence=' '.join(st)\n",
    "        segOutput.write(sentence + '\\n')\n",
    "segOutput.close()       \n",
    "\n",
    "#%%\n",
    "\n",
    "data_all=[]\n",
    "read_file =['tokens.', 'pos.', 'ner.' , 'dep.', 'seg.']\n",
    "for f in read_file:\n",
    "    thefile = open(path + '/train.' + f + 'txt','r')\n",
    "    i=0\n",
    "    for line in thefile:\n",
    "        arr = line.rstrip('\\n').split(' ')\n",
    "        if not data_all:\n",
    "            data_all= [[] for t in range(235972)]\n",
    "        if not data_all[i]:\n",
    "            data_all[i] = [[] for t in range(len(arr))]\n",
    "        for j,ele in enumerate(arr):\n",
    "            data_all[i][j].append(ele)\n",
    "        i=i+1\n",
    "# merge phrases <Hu Jintao>-><Hu_Jintao>, not PERSON PERSON, Hu PERSON, PERSON Jintao\n",
    "f='phrase.'\n",
    "thefile = open(path + '/train.' + f + 'txt','w')\n",
    "merged_data=[[] for i in range(len(data_all))]\n",
    "for j, sentence in enumerate(data_all):\n",
    "    for i, word in enumerate(sentence):\n",
    "        newWord=word[0]\n",
    "        if (word[2] == 'O' or word[2] == 'MISC') and word[4] =='O' or i==0:\n",
    "            merged_data[j].append(newWord)\n",
    "            continue\n",
    "        else:\n",
    "            if word[4] !='O' and word[4] !=sentence[i-1][4] and (word[2] == 'O' or word[2] == 'MISC'):\n",
    "                merged_data[j].append(newWord)\n",
    "            elif  word[4] !='O' and word[4] ==sentence[i-1][4] or (word[2] != 'O' and word[2] != 'MISC' and word[2] ==sentence[i-1][2]):\n",
    "                merged_data[j][-1]='_'.join((merged_data[j][-1],newWord))\n",
    "            elif word[2] != 'O' and word[2] != 'MISC' and word[2] !=sentence[i-1][2] :\n",
    "                newWord='_'.join((word[2],newWord))\n",
    "                merged_data[j].append(newWord)\n",
    "    merged_data[j]=' '.join(merged_data[j])\n",
    "    thefile.write(merged_data[j]+'\\n')\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tree class\n",
    "from collections import deque\n",
    "class SnTree(object):\n",
    "    #global cc_words\n",
    "    #cc_words=['and','or','but']\n",
    "    def __init__(self, sentence_tree):\n",
    "        #self.order = []     #visited order \n",
    "        self.tree = sentence_tree\n",
    "        self.root=-1\n",
    "        self.subtrees={}\n",
    "        self.treeEncode = {}\n",
    "        \n",
    "    def set_root(self, r_id):\n",
    "        self.root=r_id\n",
    "           \n",
    "            \n",
    "    def encode_tree(self, r, coding):\n",
    "        self.treeEncode[r]=coding\n",
    "        #print(r, self.tree[r][-1])                \n",
    "        for i,p in enumerate(self.tree[r][-1]):\n",
    "            #print(p)\n",
    "            new_coding=coding+chr(ord('A') + i)\n",
    "            self.encode_tree(r=p,coding=new_coding)\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    def split_conj(self):\n",
    "        search_queue = deque()\n",
    "        search_queue += list(self.subtrees.items())\n",
    "        #print('seachque',search_queue)\n",
    "        # split on each subtree\n",
    "        while search_queue:\n",
    "            tempSen=list(search_queue.popleft()[1])\n",
    "            sentence = sorted(tempSen)\n",
    "            #print('sentence',sentence)\n",
    "            not_position= -1\n",
    "            neither_position=-1\n",
    "            for i,word in enumerate(sentence):\n",
    "                if type(word)==float:\n",
    "                    break\n",
    "                #if two vb conjunction\n",
    "                #second vb's dep is conj and its parent is verb\n",
    "                if self.tree[word][5]=='not': \n",
    "                    not_position=word\n",
    "                if self.tree[word][5]=='neither':\n",
    "                    neither_position=word\n",
    "                \n",
    "                if self.tree[word][2]=='VERB' and self.tree[word][3]=='conj' and self.tree[self.tree[word][4]][2]=='VERB' and self.tree[word][4] in sentence:\n",
    "                    verb1=self.tree[word][4]\n",
    "                    verb2=word\n",
    "                    break_point=-1\n",
    "                    \n",
    "                    difSub=False \n",
    "                    difObj=False\n",
    "                    noObj1=True\n",
    "                    noObj2=True\n",
    "                    for w2 in self.tree[word][-1]:\n",
    "                        if 'nsubj' in self.tree[w2][3]: #do not share the same subject\n",
    "                            difSub=True\n",
    "                            #print('found subject for the second verb', w2)\n",
    "                        if self.tree[w2][3]=='dobj':\n",
    "                            noObj2=False #has object\n",
    "                        \n",
    "\n",
    "                    for w1 in self.tree[self.tree[word][4]][-1]:\n",
    "                        \n",
    "                        if self.tree[w1][3]=='dobj':\n",
    "                            noObj1=False #has object\n",
    "                            if w1<word: #do not share the same object\n",
    "                                difObj = True\n",
    "                            #print('found object for the first verb',w1)\n",
    "                        if w1>verb1 and w1<word and (self.tree[w1][2]=='ADV' or self.tree[w1][2]=='ADP'):\n",
    "                            difObj = True\n",
    "                        if self.tree[w1][3]=='cc' and w1<word and w1 in sentence and w1>verb1:\n",
    "                            break_point=w1\n",
    "                            #print('break point', break_point)\n",
    "                        \n",
    "                    while(False):        \n",
    "                    #while(i>0 and break_point<0):\n",
    "                        i-=1\n",
    "                        if self.tree[sentence[i]][1]==',' or self.tree[sentence[i]][1]==';':\n",
    "                            break_point=sentence[i]\n",
    "                            #print('break point', break_point)\n",
    "                    if break_point<0:\n",
    "                        break\n",
    "                    sent1=[]\n",
    "                    sent2=[]\n",
    "                    #if difSub and (difObj or (noObj1 and noObj2)):\n",
    "                    if difSub :\n",
    "                        #not share subject or object. safe to split\n",
    "                        #split at cc\n",
    "                        for w in sentence:\n",
    "                            if w < break_point:\n",
    "                                sent1.append(w)\n",
    "                            elif w == break_point:\n",
    "                                continue\n",
    "                            else:\n",
    "                                sent2.append(w)\n",
    "                    #elif difSub==False and (difObj or (noObj1 and noObj2)):\n",
    "                    else:\n",
    "                        #same subject, different object\n",
    "                        for j,w in enumerate(sentence):\n",
    "                            if w < verb1:\n",
    "                                if (w == not_position and self.tree[sentence[j+1]][1]=='only') or ( sentence[j-1]== not_position and self.tree[w][1]=='only'):\n",
    "                                    #not only, but also\n",
    "                                    continue\n",
    "                                if self.tree[w][1]=='both' or self.tree[w][1]=='either':\n",
    "                                    #not only, but also\n",
    "                                    continue\n",
    "                                elif w == not_position and self.tree[break_point][1] == 'but' or (self.tree[w][3]=='advmod' and self.tree[w][4]==verb1 and sentence.index(verb2)-sentence.index(break_point)>1):\n",
    "                                    #not , but\n",
    "                                    # adverb\n",
    "                                    sent1.append(w)\n",
    "                                elif w == not_position and self.tree[break_point][1] == 'or':\n",
    "                                    #not , or\n",
    "                                    sent1.append(w)\n",
    "                                    sent2.append(w)\n",
    "                                elif w == neither_position:\n",
    "                                    #neither_nor/or\n",
    "                                    sent1.append(w+0.5)\n",
    "                                    sent2.append(w+0.5)\n",
    "                                else:\n",
    "                                    sent1.append(w)\n",
    "                                    sent2.append(w)\n",
    "                            elif w<break_point:\n",
    "                                sent1.append(w)\n",
    "                            elif w==break_point:\n",
    "                                continue\n",
    "                            else:\n",
    "                                if (self.tree[w][1]=='also' and self.tree[break_point][1]=='but' and sentence[j-1]==break_point):\n",
    "                                    continue\n",
    "                                sent2.append(w)\n",
    "                                \n",
    "                    \n",
    "                    \n",
    "                    self.subtrees[verb1]=set(sent1)\n",
    "                    self.subtrees[verb2]=set(sent2)\n",
    "                    #elif difSub :\n",
    "                    #print(verb2, sent2)\n",
    "                    search_queue+= [(verb2, set(sent2)) ]  \n",
    "                    break    \n",
    "                    \n",
    "                #second vb's dep is conj and its parent's dep is acomp  \n",
    "                #if word[1][3]=='conj' and self.tree[word[1][4]][3]=='acomp':\n",
    "\n",
    "    \n",
    "    def split_noun(self, r):\n",
    "        #print('start')\n",
    "        if r !=-1:\n",
    "            search_queue = deque()\n",
    "            search_queue.append(r)\n",
    "            self.subtrees[r]=set()\n",
    "            #print(search_queue)\n",
    "        else:\n",
    "            print('root is None')\n",
    "            return -1\n",
    "\n",
    "        while search_queue:\n",
    "            person = search_queue.popleft()\n",
    "            #print(person)\n",
    "\n",
    "            if (not person in self.subtrees[r]) and (person in self.tree.keys()):\n",
    "                if (self.tree[person][2] != 'NOUN' and self.tree[person][2] != 'PROPN') or person ==r :\n",
    "                    search_queue += self.tree[person][-1]\n",
    "                    self.subtrees[r].add(person)\n",
    "                else:\n",
    "                    self.subtrees[r].add(person)\n",
    "                    self.split_noun(person)\n",
    "    \n",
    "    def concentrate_subtrees(self,windowsize_min,windowsize_max):\n",
    "        keys=list(self.subtrees.keys())\n",
    "        for k in reversed(keys):\n",
    "            if len(self.subtrees[k]) < windowsize_min:\n",
    "                #print(k,self.subtrees[k])\n",
    "                for key in self.subtrees:\n",
    "                    if k in self.subtrees[key]:\n",
    "                        #print(key, self.subtrees[key])\n",
    "                        mergeLen=len(self.subtrees[key])+ len(self.subtrees[k])\n",
    "                        if mergeLen<windowsize_max or len(self.subtrees[k])<3:\n",
    "                            self.subtrees[key].update(self.subtrees[k])\n",
    "                            if k!=key:\n",
    "                                del self.subtrees[k]\n",
    "                        break\n",
    "                        \n",
    "    \n",
    "    def subsentecne(self):\n",
    "        subs=[]\n",
    "        #print(self.subtrees)\n",
    "        for key in self.subtrees.keys():\n",
    "        #for key in sorted(self.subtrees.keys()):\n",
    "            subs.append([])\n",
    "            sentence_by_id = sorted(list(self.subtrees[key]))\n",
    "            subs[-1].append(self.tree[key][5])\n",
    "            subs[-1].append('\\t')\n",
    "            for w in sentence_by_id:\n",
    "                if type(w) == float:\n",
    "                    subs[-1].append('not')\n",
    "                else:\n",
    "                    if 'CHEMICAL' in self.tree[w][1] or 'GENE' in self.tree[w][1] or 'SPECIES' in self.tree[w][1] or 'DISEASE' in self.tree[w][1]:\n",
    "                        subs[-1].append(self.tree[w][1])\n",
    "                    else:\n",
    "                        subs[-1].append(self.tree[w][5])\n",
    "                #print(self.tree[w][1])\n",
    "            subs[-1]=' '.join(subs[-1])\n",
    "        return subs\n",
    "    def subpos(self):\n",
    "        subs=[]\n",
    "        for key in self.subtrees.keys():\n",
    "        #for key in sorted(self.subtrees.keys()):\n",
    "            subs.append([])\n",
    "            sentence_by_id = sorted(list(self.subtrees[key]))\n",
    "            subs[-1].append(self.tree[key][2])\n",
    "            subs[-1].append('\\t')\n",
    "            for w in sentence_by_id:\n",
    "                if type(w) == float:\n",
    "                    subs[-1].append('not')\n",
    "                else:\n",
    "                    subs[-1].append(self.tree[w][2])\n",
    "                \n",
    "            subs[-1]=' '.join(subs[-1])\n",
    "        return subs\n",
    "    \n",
    "    def subencode(self):\n",
    "        #print(self.treeEncode)\n",
    "        subs=[]\n",
    "        #for key in sorted(self.subtrees.keys()):\n",
    "        for key in self.subtrees.keys():    \n",
    "            subs.append([])\n",
    "            sentence_by_id = sorted(list(self.subtrees[key]))\n",
    "            subs[-1].append(self.treeEncode[key])\n",
    "            #print(self.treeEncode[key])\n",
    "            subs[-1].append('\\t')\n",
    "            for w in sentence_by_id:\n",
    "                if type(w) == float:\n",
    "                    subs[-1].append(self.treeEncode[key]+'A')\n",
    "                else:\n",
    "                    subs[-1].append(self.treeEncode[w])\n",
    "                    #print(self.treeEncode[w])\n",
    "                \n",
    "            subs[-1]=' '.join(subs[-1])\n",
    "        return subs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre_treatment of CHEMICAL can decrease the overexpression of GENE and GENE induced by CHEMICAL\n",
      "0 Pre_treatment PROPN nsubj 30 pre_treatment [30] [14]\n",
      "14 of ADP prep 0 of [0, 30] [17]\n",
      "17 CHEMICAL PROPN pobj 14 chemical [14, 0, 30] []\n",
      "26 can VERB aux 30 can [30] []\n",
      "30 decrease VERB ROOT 30 decrease [] [0, 26, 43]\n",
      "39 the DET det 43 the [43, 30] []\n",
      "43 overexpression NOUN dobj 30 overexpression [30] [39, 58]\n",
      "58 of ADP prep 43 of [43, 30] [61]\n",
      "61 GENE PROPN pobj 58 gene [58, 43, 30] [66, 70]\n",
      "66 and CCONJ cc 61 and [61, 58, 43, 30] []\n",
      "70 GENE PROPN conj 61 gene [61, 58, 43, 30] [75]\n",
      "75 induced VERB acl 70 induce [70, 61, 58, 43, 30] [83]\n",
      "83 by ADP agent 75 by [75, 70, 61, 58, 43, 30] [86]\n",
      "86 CHEMICAL PROPN pobj 83 chemical [83, 75, 70, 61, 58, 43, 30] []\n",
      "['decrease \\t pre_treatment can decrease overexpression', 'pre_treatment \\t pre_treatment of CHEMICAL', 'chemical \\t CHEMICAL', 'overexpression \\t the overexpression of GENE', 'gene \\t GENE and GENE', 'gene \\t GENE induce by CHEMICAL', 'chemical \\t CHEMICAL', 'VERB \\t PROPN VERB VERB NOUN', 'PROPN \\t PROPN ADP PROPN', 'PROPN \\t PROPN', 'NOUN \\t DET NOUN ADP PROPN', 'PROPN \\t PROPN CCONJ PROPN', 'PROPN \\t PROPN VERB ADP PROPN', 'PROPN \\t PROPN', 'A \\t AA AB A AC', 'AA \\t AA AAA AAAA', 'AAAA \\t AAAA', 'AC \\t ACA AC ACB ACBA', 'ACBA \\t ACBA ACBAA ACBAB', 'ACBAB \\t ACBAB ACBABA ACBABAA ACBABAAA', 'ACBABAAA \\t ACBABAAA']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "subSentence=[]\n",
    "sentence_tree=dict()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "j=198728\n",
    "#doc = nlp(merged_data[j])\n",
    "line='Pre_treatment of CHEMICAL can decrease the overexpression of GENE and GENE induced by CHEMICAL'\n",
    "doc=nlp(line)\n",
    "print(line)\n",
    "for i, token in enumerate(doc):\n",
    "    sentence_tree[token.idx]=[i, token.text,token.pos_, token.dep_,token.head.idx,token.lemma_, [child.idx for child in token.ancestors], [child.idx for child in token.children]]\n",
    "    if token.dep_ =='ROOT':\n",
    "        root_id=token.idx\n",
    "    print(token.idx, token.text,token.pos_, token.dep_,token.head.idx,token.lemma_, [child.idx for child in token.ancestors],[child.idx for child in token.children])\n",
    "\n",
    "sentence_try1=SnTree(sentence_tree)\n",
    "sentence_try1.root=root_id\n",
    "sentence_try1.encode_tree(root_id,'A')\n",
    "\n",
    "sentence_try1.split_noun(root_id)\n",
    "sentence_try1.split_conj()\n",
    "#sentence_try1.concentrate_subtrees(4,10)\n",
    "subSentence += sentence_try1.subsentecne()\n",
    "subSentence += sentence_try1.subpos()\n",
    "subSentence += sentence_try1.subencode()\n",
    "print(subSentence)\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "svg = displacy.render(doc, style='dep')\n",
    "output_path = open('parse.svg','w', encoding='utf-8')\n",
    "output_path.write(svg)\n",
    "output_path.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# bio data replace CHEMICAL,GENE,...\n",
    "path = 'OpenIE/Stanford-OpenIE/bio'\n",
    "merged_data_dict=dict()\n",
    "merged_data=[]\n",
    "read_file =['ner.']\n",
    "entity_dict=dict()\n",
    "entity_id=0\n",
    "for f in read_file:\n",
    "    thefile = open(path + '/train.' + f + 'txt','r')\n",
    "    for line in thefile:\n",
    "        sn = line.rstrip('.\\n')\n",
    "        if len(sn)<2:\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            arr=sn.split(' ')\n",
    "            tokens=[]\n",
    "            j=0\n",
    "            for i in range(len(arr)):\n",
    "                if 'CHEMICAL' in arr[i] or 'GENE' in arr[i] or 'SPECIES' in arr[i] or 'DISEASE' in arr[i]:\n",
    "                    punct=''\n",
    "                    if not arr[i][-1].isalpha():\n",
    "                        punct=arr[i][-1]\n",
    "                    centity=arr[i].split('-')\n",
    "                    original_entity='-'.join(centity[0:-1])\n",
    "                    temp_entity=centity[0].split('_')[0] + str(j)\n",
    "                    entity_dict[temp_entity]=original_entity\n",
    "                    if len(centity)>1:\n",
    "                        arr[i]=temp_entity + '_'+centity[-1]\n",
    "                    else:\n",
    "                        arr[i]=temp_entity + punct\n",
    "                    \n",
    "                    tokens.append(arr[i])\n",
    "                    j+=1\n",
    "                elif '-' in arr[i]:\n",
    "                    t = arr[i].replace('-','_')\n",
    "                    tokens.append(t)\n",
    "                else:\n",
    "                    tokens.append(arr[i])\n",
    "            merged_data.append(' '.join(tokens))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio tag data replace CHEMICAL,GENE,...\n",
    "path = 'bio2'\n",
    "merged_data_dict=dict()\n",
    "merged_data=[]\n",
    "read_file =['ner.']\n",
    "ner_types=['DISEASE','GENE', 'CHEMICAL','PATHWAY','SNP','BP','MF','CC']\n",
    "entity_dict=dict()\n",
    "entity_id=0\n",
    "j=0\n",
    "line_number=0\n",
    "line_id=[]\n",
    "for f in read_file:\n",
    "    thefile = open(path + '/corpus_tagged.txt','r')\n",
    "    for line in thefile:\n",
    "        line_number=line_number+1\n",
    "        sn = line.rstrip('.\\n')           \n",
    "        arr=sn.split(' ')\n",
    "        tokens=[]\n",
    "        if len(arr)<5:\n",
    "            continue\n",
    "        for i in range(len(arr)):\n",
    "            for ner in ner_types:\n",
    "                start_with_dash=False\n",
    "                if ner in arr[i]:\n",
    "                    punct=''\n",
    "                    if arr[i][-2:]=='),' or arr[i][-2:]==');':\n",
    "                        punct=arr[i][-2:]\n",
    "                    elif arr[i][-1]==',' or arr[i][-1]==')' or arr[i][-1]=='\"' or arr[i][-1]==';':\n",
    "                        punct=arr[i][-1]\n",
    "\n",
    "                    centity=arr[i].split('-')\n",
    "                    if not centity[0]:\n",
    "                        start_with_dash=True\n",
    "                        centity.pop(0)\n",
    "                    temp_entity=centity[0].split('_')[0] + str(j)\n",
    "\n",
    "                    if ner not in temp_entity:\n",
    "                        for nerner in ner_types:\n",
    "                            if nerner in temp_entity:\n",
    "                                temp_entity=temp_entity.replace(nerner,ner)\n",
    "\n",
    "                    t1=ner+ str(j)\n",
    "\n",
    "\n",
    "                    if len(centity)>1:\n",
    "                        original_entity='-'.join(centity[0:-1])\n",
    "                        arr[i]=temp_entity + '_'+centity[-1]\n",
    "                    else:\n",
    "                        arr[i]=temp_entity + punct\n",
    "                        if start_with_dash:\n",
    "                            arr[i]='-'+arr[i]\n",
    "                        original_entity=centity\n",
    "                    entity_dict[t1]=original_entity\n",
    "\n",
    "\n",
    "                    j+=1\n",
    "                    break\n",
    "            if '-' in arr[i]:\n",
    "                t = arr[i].replace('-','_')\n",
    "                tokens.append(t)\n",
    "            else:\n",
    "                tokens.append(arr[i])\n",
    "        merged_data.append(' '.join(tokens))\n",
    "        line_id.append(str(line_number))\n",
    "print('reading_in:done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading_in:done\n"
     ]
    }
   ],
   "source": [
    "# bio data replace CHEMICAL,GENE,...\n",
    "path = 'bio'\n",
    "merged_data_dict=dict()\n",
    "merged_data=[]\n",
    "read_file =['ner.']\n",
    "ner_types=['DISEASE','GENE', 'CHEMICAL','SPECIES']\n",
    "entity_dict=dict()\n",
    "entity_id=0\n",
    "j=0\n",
    "line_number=0\n",
    "line_id=[]\n",
    "for f in read_file:\n",
    "    thefile = open(path + '/train1.' + f + 'txt','r')\n",
    "    for line in thefile:\n",
    "        line_number=line_number+1\n",
    "        sn = line.rstrip('.\\n')           \n",
    "        arr=sn.split(' ')\n",
    "        tokens=[]\n",
    "        if len(arr)<5:\n",
    "            continue\n",
    "        for i in range(len(arr)):\n",
    "            for ner in ner_types:\n",
    "                start_with_dash=False\n",
    "                if ner in arr[i]:\n",
    "                    punct=''\n",
    "                    if arr[i][-2:]=='),' or arr[i][-2:]==');':\n",
    "                        punct=arr[i][-2:]\n",
    "                    elif arr[i][-1]==',' or arr[i][-1]==')' or arr[i][-1]=='\"' or arr[i][-1]==';':\n",
    "                        punct=arr[i][-1]\n",
    "\n",
    "                    centity=arr[i].split('-')\n",
    "                    if not centity[0]:\n",
    "                        start_with_dash=True\n",
    "                        centity.pop(0)\n",
    "                    temp_entity=centity[0].split('_')[0] + str(j)\n",
    "\n",
    "                    if ner not in temp_entity:\n",
    "                        for nerner in ner_types:\n",
    "                            if nerner in temp_entity:\n",
    "                                temp_entity=temp_entity.replace(nerner,ner)\n",
    "\n",
    "                    t1=ner+ str(j)\n",
    "\n",
    "\n",
    "                    if len(centity)>1:\n",
    "                        original_entity='-'.join(centity[0:-1])\n",
    "                        arr[i]=temp_entity + '_'+centity[-1]\n",
    "                    else:\n",
    "                        arr[i]=temp_entity + punct\n",
    "                        if start_with_dash:\n",
    "                            arr[i]='-'+arr[i]\n",
    "                        original_entity=centity\n",
    "                    entity_dict[t1]=original_entity\n",
    "\n",
    "\n",
    "                    j+=1\n",
    "                    break\n",
    "            if '-' in arr[i]:\n",
    "                t = arr[i].replace('-','_')\n",
    "                tokens.append(t)\n",
    "            else:\n",
    "                tokens.append(arr[i])\n",
    "        merged_data.append(' '.join(tokens))\n",
    "        line_id.append(str(line_number))\n",
    "print('reading_in:done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "g = 'extraction_dictionary.'\n",
    "path = 'bio'\n",
    "ex_id='/train2.'\n",
    "entityOutput = path +ex_id+ g + 'pickle'\n",
    "with open(entityOutput, 'wb') as handle:\n",
    "    pickle.dump(entity_dict, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2000\n",
      "4000\n",
      "6000\n",
      "8000\n",
      "10000\n",
      "12000\n",
      "14000\n",
      "16000\n",
      "18000\n",
      "20000\n",
      "22000\n",
      "24000\n",
      "26000\n",
      "28000\n",
      "30000\n",
      "32000\n",
      "34000\n",
      "36000\n",
      "38000\n",
      "40000\n",
      "42000\n",
      "44000\n",
      "46000\n",
      "48000\n",
      "50000\n",
      "52000\n",
      "54000\n",
      "56000\n",
      "58000\n",
      "60000\n",
      "62000\n",
      "64000\n",
      "66000\n",
      "68000\n",
      "70000\n",
      "72000\n",
      "74000\n",
      "76000\n",
      "78000\n",
      "80000\n",
      "82000\n",
      "84000\n",
      "86000\n",
      "88000\n",
      "90000\n",
      "92000\n",
      "94000\n",
      "96000\n",
      "98000\n",
      "100000\n",
      "102000\n",
      "104000\n",
      "106000\n",
      "108000\n",
      "110000\n",
      "112000\n",
      "114000\n",
      "116000\n",
      "118000\n",
      "120000\n",
      "122000\n",
      "124000\n",
      "126000\n",
      "128000\n",
      "130000\n",
      "132000\n",
      "134000\n",
      "136000\n",
      "138000\n",
      "140000\n",
      "142000\n",
      "144000\n",
      "146000\n",
      "148000\n",
      "150000\n",
      "152000\n",
      "154000\n",
      "156000\n",
      "158000\n",
      "160000\n",
      "162000\n",
      "164000\n",
      "166000\n",
      "168000\n",
      "170000\n",
      "172000\n",
      "174000\n",
      "176000\n",
      "178000\n",
      "180000\n",
      "182000\n",
      "184000\n",
      "186000\n",
      "188000\n",
      "190000\n",
      "192000\n",
      "194000\n",
      "196000\n",
      "198000\n",
      "200000\n",
      "202000\n",
      "204000\n",
      "206000\n",
      "208000\n",
      "210000\n",
      "212000\n",
      "214000\n",
      "216000\n",
      "218000\n",
      "220000\n",
      "222000\n",
      "224000\n",
      "226000\n",
      "228000\n",
      "230000\n",
      "232000\n",
      "234000\n",
      "236000\n",
      "238000\n",
      "240000\n",
      "242000\n",
      "244000\n",
      "246000\n",
      "248000\n",
      "250000\n",
      "252000\n",
      "254000\n",
      "256000\n",
      "258000\n",
      "260000\n",
      "262000\n",
      "264000\n",
      "266000\n",
      "268000\n",
      "270000\n",
      "272000\n",
      "274000\n",
      "276000\n",
      "278000\n",
      "280000\n",
      "282000\n",
      "284000\n",
      "286000\n",
      "288000\n",
      "290000\n",
      "292000\n",
      "294000\n",
      "296000\n",
      "298000\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "path = 'bio'\n",
    "write=True\n",
    "ex_id='/train2.'\n",
    "f = 'subSentence.'\n",
    "subOutput = open(path +ex_id+ f + 'txt', 'w')\n",
    "'''\n",
    "g = 'extraction_dictionary.'\n",
    "entityOutput = path +ex_id+ g + 'pickle'\n",
    "with open(entityOutput, 'wb') as handle:\n",
    "    pickle.dump(entity_dict, handle)\n",
    "'''\n",
    "h = 'subPOS.'\n",
    "posOutput = open(path +ex_id+ h + 'txt', 'w')\n",
    "l = 'subEncode.'\n",
    "encodeOutput = open(path +ex_id+ l + 'txt', 'w')\n",
    "\n",
    "subSentence=[]\n",
    "subSentence_pos=[]\n",
    "subSentence_encode=[]\n",
    "for j in range(len(merged_data)):\n",
    "#    j=17\n",
    "    doc = nlp(merged_data[j])\n",
    "    sentence_tree={}\n",
    "    sentence_visit=[]\n",
    "    for i, token in enumerate(doc):\n",
    "        sentence_tree[token.idx]=[i, token.text,token.pos_, token.dep_,token.head.idx,token.lemma_, [child.idx for child in token.ancestors],[child.idx for child in token.children]]\n",
    "        if token.dep_ =='ROOT':\n",
    "            root_id=token.idx\n",
    "        #print(token.text, token.idx, token.dep_, token.head.text, token.pos_,[child.idx for child in token.children])\n",
    "\n",
    "    sentence_try=SnTree(sentence_tree)\n",
    "\n",
    "    sentence_try.root=root_id\n",
    "    sentence_try.encode_tree(root_id,'A')\n",
    "    sentence_try.split_noun(root_id)\n",
    "    sentence_try.split_conj()\n",
    "    #sentence_try.concentrate_subtrees(4,10)\n",
    "    #pprint.pprint(sentence_try.subtrees)\n",
    "    subSentence += sentence_try.subsentecne()\n",
    "    if j % 2000 == 0:\n",
    "        print(j)\n",
    "    subSentence_pos += sentence_try.subpos()\n",
    "    subSentence_encode += sentence_try.subencode()\n",
    "    #print(subSentence)\n",
    "    if write:\n",
    "        subOutput.write(line_id[j] + '\\n')\n",
    "        for subs in sentence_try.subsentecne():\n",
    "            subOutput.write(subs + '\\n')        \n",
    "        subOutput.write('\\n')\n",
    "        posOutput.write(line_id[j] + '\\n')\n",
    "        for subs in sentence_try.subpos():\n",
    "            posOutput.write(subs + '\\n')        \n",
    "        posOutput.write('\\n')\n",
    "        encodeOutput.write(line_id[j] + '\\n')\n",
    "        for subs in sentence_try.subencode():\n",
    "            encodeOutput.write(subs + '\\n')        \n",
    "        encodeOutput.write('\\n')\n",
    "        \n",
    "subOutput.close()\n",
    "posOutput.close()\n",
    "encodeOutput.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import threading\n",
    "import collections\n",
    "import math\n",
    "\n",
    "ARR = multiprocessing.Array('b', 235969 + 1)\n",
    "DEBUG = False\n",
    "MIN_SUP = 5\n",
    "\n",
    "candidates=[]\n",
    "current_words = set()\n",
    "words_freq = defaultdict(int)\n",
    "\n",
    "subOutput = open(path +'/train.' + f + 'txt', 'r')\n",
    "snID=0\n",
    "for line in subOutput:\n",
    "    sn = line.rstrip(' .\\n').split(' \\t ')\n",
    "    snID+=1\n",
    "    if len(sn)<2:\n",
    "        continue\n",
    "    else:\n",
    "        temp=dict()\n",
    "        temp['sent_id']=snID\n",
    "        arr=sn[1].split(' ')\n",
    "        for i in range(len(arr)):\n",
    "            if 'CHEMICAL' in arr[i] or 'GENE' in arr[i] or 'SPECIES' in arr[i] or 'DISEASE' in arr[i]:\n",
    "                arr[i]=arr[i].split('_')[0]\n",
    "                words_freq[arr[i]] += 1\n",
    "                current_words.add(arr[i])\n",
    "            \n",
    "            else:\n",
    "                arr[i]=arr[i].lower()\n",
    "                words_freq[arr[i]] += 1\n",
    "                current_words.add(arr[i])\n",
    "        temp['sent']=arr\n",
    "        candidates.append(temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def is_subseq(sequence, subsequece):\n",
    "    if len(sequence) < len(subsequece):\n",
    "        return False\n",
    "    it = iter(sequence)\n",
    "    return all(c in it for c in subsequece)\n",
    "\n",
    "def flatten(l):\n",
    "    return list(itertools.chain.from_iterable(l))\n",
    "\n",
    "def flatten2(l):\n",
    "    for el in l:\n",
    "        if isinstance(el, collections.Iterable) and not isinstance(el, (int, str, bytes)):\n",
    "            yield from flatten2(el)\n",
    "        else:\n",
    "            yield el\n",
    "\n",
    "\n",
    "\n",
    "def match_sent(candidates):\n",
    "    matched_sents = set()\n",
    "    match_target = get_subseq_to_match(candidate['sent'])\n",
    "    for j, to_match in enumerate(candidates):\n",
    "        if is_subseq(to_match['expanded'], match_target):\n",
    "            matched_sents.add(to_match['sent_id'])\n",
    "            if len(matched_sents) > MIN_SUP:\n",
    "                break\n",
    "    \n",
    "\n",
    "def match(candidates, words_freq, original_candidates):\n",
    "    print(len(candidates))\n",
    "    global arr\n",
    "    patterns = defaultdict(set)\n",
    "    linked = set()\n",
    "    result = []\n",
    "\n",
    "    # main loop for MATCH\n",
    "    # for cnt, candidate in tqdm(enumerate(candidates)):\n",
    "    for cnt, candidate in enumerate(candidates):\n",
    "        if DEBUG and cnt == 10:\n",
    "            break\n",
    "\n",
    "        if cnt % 100 == 0:\n",
    "            print(threading.current_thread().name, time.strftime(\"%m-%d %H:%M:%S\"), cnt)\n",
    "        # if candidate['sent_id'] in linked:\n",
    "        #     continue\n",
    "        if arr[candidate['sent_id']] != 0:\n",
    "            continue\n",
    "\n",
    "        freq = [words_freq[s] for s in candidate['sent']]\n",
    "        arg_freq_sort = np.argsort(freq)\n",
    "        # print(candidate)\n",
    "        # print(freq)\n",
    "        # print(arg_freq_sort)\n",
    "\n",
    "        matched = False\n",
    "        for replace_pos in arg_freq_sort:\n",
    "            if matched:\n",
    "                break\n",
    "                    \n",
    "            #for i in range(5):\n",
    "            for i in [0]: \n",
    "                # match\n",
    "                matched_sents = set()\n",
    "                matched_pairs = defaultdict(int)\n",
    "\n",
    "                match_target = candidate['sent']\n",
    "                for j, to_match in enumerate(original_candidates):\n",
    "                    if is_subseq(to_match['sent'], match_target):\n",
    "\n",
    "                        matched_sents.add(to_match['sent_id'])\n",
    "                        # adding count\n",
    "                        \n",
    "                        # if len(matched_sents) > MIN_SUP:\n",
    "                        #     break\n",
    "\n",
    "                # print('matched', matched_sents)\n",
    "                # ***** NOT MATCHING - START REPLACE *****\n",
    "                if len(matched_sents) < MIN_SUP:\n",
    "                    # print('replace_pos', replace_pos)\n",
    "                    # print(candidate['sent'])\n",
    "                    candidate['sent'][replace_pos]=''\n",
    "                    \n",
    "                    \n",
    "      \n",
    "                else:\n",
    "                    matched = True\n",
    "                    # print(candidate['sent'])\n",
    "                    for s in range(len(candidate['sent'])):\n",
    "                        if candidate['sent'][s]=='':\n",
    "                            candidate['sent'][s]='*'\n",
    "                    key = ' '.join([s for s in candidate['sent']])\n",
    "                    if key not in patterns:\n",
    "                        patterns[key]=matched_sents\n",
    "                    # linked.union(matched_sents)\n",
    "\n",
    "                    \n",
    "                        cur_pattern = {\n",
    "                            'pattern': key,\n",
    "                            'freq': len(patterns[key]),\n",
    "                            'matched': patterns[key],\n",
    "                        }\n",
    "                        result.append(cur_pattern)\n",
    "                    break\n",
    "\n",
    "    return result\n",
    "def run_parallel(\n",
    "    candidates,\n",
    "    words_freq,\n",
    "    num_workers = 1,\n",
    "    save = False,\n",
    "):\n",
    "    num_workers+=1\n",
    "    pool = multiprocessing.Pool(processes=num_workers)\n",
    "\n",
    "    num_lines = len(candidates)\n",
    "    batch_size = math.floor(num_lines/(num_workers-1))\n",
    "    print(\"batch_size: %d\" % batch_size)\n",
    "\n",
    "    start_pos = [i*batch_size for i in range(0, num_workers)]\n",
    "\n",
    "    results = [pool.apply_async(match, args=(\n",
    "        list(candidates[start:start+batch_size]), \n",
    "        words_freq,\n",
    "        candidates,\n",
    "    )) for i, start in enumerate(start_pos)]\n",
    "\n",
    "    results = [p.get() for p in results]\n",
    "    \n",
    "    # res={}\n",
    "    # for r in results:\n",
    "    #     res.update(r)\n",
    "    res = flatten(results)\n",
    "    # print(res)\n",
    "    if(save):\n",
    "        json.dump(res, open('pattern_result_test1210.json', 'w'), indent=1)\n",
    "        # self.save_to_file(res, \"probase_local_parallel.p\")\n",
    "# old code\n",
    "# new code at server\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#run_parallel(candidates, words_freq, num_workers=20, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def frequentPatternMining(tokens,patternOutputFilename, threshold):\n",
    "    dict_token = {}\n",
    "\n",
    "    tokensNumber = len(tokens)\n",
    "    for i in range(tokensNumber):\n",
    "        token = tokens[i]\n",
    "        if token == '$':\n",
    "            continue\n",
    "        if token in dict_token:\n",
    "            dict_token[token].append(i)\n",
    "        else:\n",
    "            dict_token[token] = [i]\n",
    "    print (\"# of distinct tokens = \", len(dict_token))\n",
    "\n",
    "    patternOutput = open(patternOutputFilename, 'w')\n",
    "\n",
    "    frequentPatterns = []\n",
    "    patternLength = 1\n",
    "    while (len(dict_token) > 0):\n",
    "        if patternLength > 10:\n",
    "            break\n",
    "        #print \"working on length = \", patternLength\n",
    "        patternLength += 1\n",
    "        newDict = {}\n",
    "        for pattern, positions in dict_token.items():\n",
    "            occurrence = len(positions)\n",
    "            if occurrence >= threshold:\n",
    "                frequentPatterns.append(pattern)\n",
    "                \n",
    "                patternOutput.write(pattern + \",\" + str(occurrence) + \"\\n\")\n",
    "                for i in positions:\n",
    "                    if i + 1 < tokensNumber:\n",
    "                        if tokens[i + 1] == '$':\n",
    "                            continue\n",
    "                        newPattern = pattern + \" \" + tokens[i + 1]\n",
    "                        if newPattern in newDict:\n",
    "                            newDict[newPattern].append(i + 1)\n",
    "                        else:\n",
    "                            newDict[newPattern] = [i + 1]\n",
    "        dict_token.clear()\n",
    "        dict_token = newDict\n",
    "    patternOutput.close()\n",
    "    return frequentPatterns\n",
    "\n",
    "path = 'OpenIE/Stanford-OpenIE/bio'\n",
    "f = 'subSentence.'\n",
    "subOutput = open(path +'/train.' + f + 'txt', 'r')\n",
    "snID=0\n",
    "tokens=[]\n",
    "for line in subOutput:\n",
    "    sn = line.rstrip(' .\\n').split(' \\t ')\n",
    "    snID+=1\n",
    "    if len(sn)<2:\n",
    "        continue\n",
    "    else:\n",
    "        temp=dict()\n",
    "        temp['sent_id']=snID\n",
    "        arr=sn[1].split(' ')\n",
    "        for i in range(len(arr)):\n",
    "            arr[i]=arr[i].split('_')\n",
    "            if 'CHEMICAL' in arr[i][0]:\n",
    "                arr[i][0]='CHEMICAL'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            elif 'GENE' in arr[i][0]:\n",
    "                arr[i][0]='GENE'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            elif 'SPECIES' in arr[i][0] :\n",
    "                arr[i][0]='SPECIES'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            elif 'DISEASE' in arr[i][0]:          \n",
    "                arr[i][0]='DISEASE'\n",
    "                tokens.append('_'.join(arr[i]))         \n",
    "            else:\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "    tokens.append('$')\n",
    "#frequentPatterns = frequentPatternMining(tokens, patternOutputFilename, threshold)\n",
    "\n",
    "f_out=path +'/train.metapattern.txt'\n",
    "frequentPatterns=frequentPatternMining(tokens,f_out,10)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
