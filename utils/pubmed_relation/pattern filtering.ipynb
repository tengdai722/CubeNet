{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def frequentPatternMining(tokens, tokensEncode, threshold, maxlen):\n",
    "    dict_token = {}\n",
    "    all_token={}\n",
    "    \n",
    "    tokensNumber = len(tokens)\n",
    "    for i in range(tokensNumber):\n",
    "        token = tokens[i]\n",
    "        if token == '$':\n",
    "            continue\n",
    "        if token in dict_token:\n",
    "            dict_token[token][0].append(i)\n",
    "            dict_token[token][1].append(1)\n",
    "        else:\n",
    "            dict_token[token] = [[i],[1]]\n",
    "    print (\"# of distinct tokens = \", len(dict_token))\n",
    "\n",
    "    token_pattern={}\n",
    "\n",
    "    frequentPatterns = []\n",
    "    patternLength = 1\n",
    "    while (len(dict_token) > 0):\n",
    "        if patternLength > maxlen:\n",
    "            break\n",
    "        print (\"working on length = \", patternLength)\n",
    "        patternLength += 1\n",
    "        newDict = {}\n",
    "        for pattern, positions_tf in dict_token.items():\n",
    "            positions=positions_tf[0]\n",
    "            valid=positions_tf[1]\n",
    "            occurrence = len(positions)\n",
    "            if occurrence >= threshold:\n",
    "                \n",
    "                if patternLength>2 and sum(valid)>threshold:\n",
    "                    for j, p in enumerate(positions):\n",
    "                        if valid[j]>0:\n",
    "                            if p in token_pattern:\n",
    "                                token_pattern[p].append(pattern)\n",
    "                            else:\n",
    "                                token_pattern[p]=[pattern]\n",
    "                                \n",
    "                    \n",
    "                    frequentPatterns.append([pattern,occurrence])\n",
    "                    all_token[pattern]=positions\n",
    "                \n",
    "                #patternOutput.write(pattern + \",\" + str(occurrence) + \"\\n\")\n",
    "                for i in positions:\n",
    "                    if i + 1 < tokensNumber:\n",
    "                        if tokens[i + 1] == '$':\n",
    "                            continue\n",
    "                        newPattern = pattern + \" \" + tokens[i + 1]\n",
    "                        invalid=False\n",
    "                        pattern_tree=tokensEncode[i-patternLength+2: i+2]\n",
    "                        pattern_root=min(pattern_tree, key=len)\n",
    "                        for t in pattern_tree:\n",
    "                            if t[0:-1] in pattern_tree or t==pattern_root:\n",
    "                                continue\n",
    "                            else:\n",
    "                                invalid=True\n",
    "                                \n",
    "                        if invalid:\n",
    "                            new_valid=0\n",
    "                        else:\n",
    "                            new_valid=1\n",
    "                         \n",
    "                        if newPattern in newDict:\n",
    "                            newDict[newPattern][0].append(i + 1)\n",
    "                            newDict[newPattern][1].append(new_valid) \n",
    "                        else:\n",
    "                            newDict[newPattern] = [[i + 1], [new_valid]]\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "        dict_token.clear()\n",
    "        dict_token = newDict\n",
    "\n",
    "    return sorted(frequentPatterns, key=lambda x:(x[0],x[1])), all_token, token_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'bio'\n",
    "f = 'subSentence.'\n",
    "f = 'new_sub1.'\n",
    "ex_id='/train2.'\n",
    "subOutput = open(path +ex_id+ f + 'txt', 'r')\n",
    "tokens=[]\n",
    "main_words=[]\n",
    "for line in subOutput:\n",
    "    sn = line.rstrip('\\n').split(' \\t ')\n",
    "    snID+=1\n",
    "    if len(sn)<2:\n",
    "        tokens.append('$')\n",
    "        main_words.append('$')\n",
    "        continue\n",
    "    else:\n",
    "        main_words.append(sn[0])\n",
    "        arr=sn[1].split(' ')\n",
    "        for i in range(len(arr)):\n",
    "           \n",
    "            arr[i]=arr[i].split('_')\n",
    "            if 'CHEMICAL' in arr[i][0]:\n",
    "                arr[i][0]='CHEMICAL'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            elif 'GENE' in arr[i][0]:\n",
    "                arr[i][0]='GENE'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            elif 'SPECIES' in arr[i][0] :\n",
    "                arr[i][0]='SPECIES'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            elif 'DISEASE' in arr[i][0]:          \n",
    "                arr[i][0]='DISEASE'\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "            \n",
    "            else:\n",
    "                tokens.append('_'.join(arr[i]))\n",
    "    tokens.append('$')\n",
    "#frequentPatterns = frequentPatternMining(tokens, patternOutputFilename, threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 'subEncode.'\n",
    "encodeOutput = open(path +ex_id+ l + 'txt', 'r')\n",
    "snID=0\n",
    "tokensEncode=[]\n",
    "for line in encodeOutput:\n",
    "    sn = line.rstrip(' .\\n').split(' \\t ')\n",
    "    snID+=1\n",
    "    if len(sn)<2:\n",
    "        if sn:\n",
    "            tokensEncode.append(sn[0])\n",
    "        else:\n",
    "            tokensEncode.append('$')\n",
    "        continue\n",
    "    else:\n",
    "        arr=sn[1].split(' ')\n",
    "        for i in range(len(arr)):\n",
    "            tokensEncode.append(arr[i])\n",
    "    tokensEncode.append('$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(tokensEncode)!=len(tokens):\n",
    "    for i in range(len(tokensEncode)):\n",
    "        if (tokensEncode[i]=='$' and tokens[i]!='$') or (tokensEncode[i]!='$' and tokens[i]=='$'):\n",
    "            print(tokens[i-10:i+10], tokens[i])\n",
    "            print(tokensEncode[i-10:i+10], tokensEncode[i])\n",
    "            print(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of distinct tokens =  110189\n",
      "working on length =  1\n",
      "working on length =  2\n",
      "working on length =  3\n",
      "working on length =  4\n",
      "working on length =  5\n",
      "working on length =  6\n",
      "working on length =  7\n",
      "working on length =  8\n",
      "working on length =  9\n",
      "working on length =  10\n"
     ]
    }
   ],
   "source": [
    "f_out=path +ex_id+'metapattern.txt'\n",
    "f_out=path +ex_id+'new_metapattern.txt'\n",
    "frequentPatterns, pattern_location_dict, exact_match_dict=frequentPatternMining(tokens,tokensEncode,10,15)\n",
    "write=True\n",
    "if write:\n",
    "    with open(f_out, 'w') as patternOutput:\n",
    "        for p in frequentPatterns:\n",
    "            line=p[0]+'\\t'+str(p[1])\n",
    "            patternOutput.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_out=path +ex_id+'pattern_match.txt'\n",
    "f_out=path +ex_id+'new_pattern_match.txt'\n",
    "k=0\n",
    "pattern=[]\n",
    "with open(f_out, 'w') as patternOutput:\n",
    "    for j in range(len(tokensEncode)):\n",
    "        \n",
    "        if tokensEncode[j]=='$' and main_words[k]=='$':\n",
    "            k+=1\n",
    "            patternOutput.write('\\n')\n",
    "        elif tokensEncode[j]=='$' and main_words[k]!='$':\n",
    "            \n",
    "            patterns='\\t'.join(pattern)\n",
    "            patternOutput.write(main_words[k] + '\\t' + patterns+'\\n')\n",
    "            pattern=[]\n",
    "            k+=1\n",
    "            #patternOutput.write('\\n')\n",
    "        elif tokensEncode[j]!='$' and tokens[j]=='$':\n",
    "            k+=1\n",
    "            patternOutput.write(tokensEncode[j] + '\\n')\n",
    "        else:\n",
    "            if j in exact_match_dict:\n",
    "                pattern_t=sorted(exact_match_dict[j], key=len, reverse=True)[0]\n",
    "                if len(pattern)==0 or not pattern[-1] in pattern_t:\n",
    "                    pattern.append(pattern_t)\n",
    "                else:\n",
    "                    pattern[-1]=pattern_t\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ['effect of']\n",
      "36 ['effect of', 'the effect of']\n",
      "1482 ['effect of', 'the effect of']\n",
      "1532 ['effect of']\n",
      "2020 ['effect of', 'the effect of']\n",
      "2912 ['effect of', 'side effect of']\n",
      "3029 ['effect of']\n",
      "4088 ['effect of', 'the effect of']\n",
      "4125 ['effect of', 'the effect of']\n",
      "4258 ['effect of', 'the effect of']\n",
      "4323 ['effect of', 'stimulatory effect of', 'the stimulatory effect of']\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "for i in exact_match_dict:\n",
    "    print(i,exact_match_dict[i])\n",
    "    j+=1\n",
    "    if j>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 'metapattern.'\n",
    "f = 'new_metapattern.'\n",
    "g = 'stopword.'\n",
    "inputPattern = open(path +ex_id+ f + 'txt', 'r')\n",
    "stopwordFile = open(path +'/' + g + 'txt', 'r')\n",
    "ner_types=['DISEASE','GENE', 'CHEMICAL','SPECIES']\n",
    "stopword=[]\n",
    "for line in stopwordFile:\n",
    "    sn = line.rstrip('\\n')\n",
    "    stopword.append(sn)\n",
    "tokens=[]\n",
    "main_words=[]\n",
    "\n",
    "multiNER = []\n",
    "singleNER = []\n",
    "phrase = []\n",
    "for line in inputPattern:\n",
    "    sn = line.rstrip('\\n').split('\\t')\n",
    "    main_pattern = sn[0]\n",
    "    nerNo=0\n",
    "    for ner in ner_types:\n",
    "        if ner in main_pattern:\n",
    "            nerNo+=1\n",
    "            r=main_pattern.split(ner)\n",
    "            nerNo+=len(r)-2\n",
    "    if nerNo<2 and nerNo>0:\n",
    "        #print(line)\n",
    "        r=main_pattern.split(' ')\n",
    "        wordNo=len(r)\n",
    "        for word in r:\n",
    "            if word in stopword:\n",
    "                wordNo-=1\n",
    "        if wordNo>1:\n",
    "            singleNER.append([main_pattern,int(sn[1])])\n",
    "    elif nerNo==0:\n",
    "        r=main_pattern.split(' ')\n",
    "        wordNo=len(r)\n",
    "        for word in r:\n",
    "            if word in stopword:\n",
    "                wordNo-=1\n",
    "        if wordNo>1:\n",
    "            phrase.append([main_pattern,int(sn[1])])\n",
    "    else:\n",
    "        multiNER.append([main_pattern,int(sn[1])])\n",
    "\n",
    "        \n",
    "singleNER.sort(key=lambda x: x[1],reverse=True)\n",
    "multiNER.sort(key=lambda x: x[1],reverse=True)\n",
    "phrase.sort(key=lambda x: x[1],reverse=True)\n",
    "singlePath=path +ex_id + 'single.pattern.' + 'txt'\n",
    "multiPath=path + ex_id + 'multi.pattern.' + 'txt'\n",
    "phrasePath=path + ex_id + 'phrase.pattern.' + 'txt'\n",
    "singlePath=path +ex_id + 'new_single.pattern.' + 'txt'\n",
    "multiPath=path + ex_id + 'new_multi.pattern.' + 'txt'\n",
    "phrasePath=path + ex_id + 'new_phrase.pattern.' + 'txt'\n",
    "with open(singlePath, 'w') as single:\n",
    "    for p in singleNER:\n",
    "        single.write(p[0]+'\\t'+str(p[1])+'\\n')\n",
    "with open(multiPath, 'w') as multi:\n",
    "    for p in multiNER:\n",
    "        multi.write(p[0]+'\\t'+str(p[1])+'\\n')     \n",
    "with open(phrasePath, 'w') as ph:\n",
    "    for p in phrase:\n",
    "        ph.write(p[0]+'\\t'+str(p[1])+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
